{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarini-py/Machine-Learning/blob/main/Linear_Regression_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIMEgWZf1Aj1"
      },
      "source": [
        "# Linear Regression - GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rJtno6aRlOPf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "class linear_regression:\n",
        "\n",
        "    '''\n",
        "    >> model = linera_regression()\n",
        "    >> model.fit(X_train, y_train)\n",
        "    >> model.preict(X_test)\n",
        "    '''\n",
        "    def __init__(self, learning_rate = 0.1, iteration = 100):\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iteration = iteration\n",
        "\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "        #self.m -> no. of training samples( rows )\n",
        "        #self.d -> no. of features( cols )\n",
        "        self.m, self.d = X_train.shape\n",
        "\n",
        "        #initializing Weights(self.W) and bias(self.b) with 0 to start with\n",
        "        self.W = np.zeros(self.d)\n",
        "        self.b = 0\n",
        "\n",
        "        #list of error to keep track of errors\n",
        "        self.errors = []\n",
        "\n",
        "        for i in range(self.iteration):\n",
        "\n",
        "            #calculating y_hat(prediction) based on initial W, b\n",
        "            y_hat = self.predict(self.X_train)\n",
        "\n",
        "            #calculating partial derivatives\n",
        "            dW = (-1) * np.dot(self.X_train.T, (self.y_train - y_hat))/self.m\n",
        "            db = (-1) * np.sum((self.y_train - y_hat))/self.m\n",
        "\n",
        "            #updating weights & bias using the above calculated derivatives\n",
        "            self.W -= self.learning_rate * dW\n",
        "            self.b -= self.learning_rate * db\n",
        "\n",
        "            error = np.mean(np.square( y_train - self.predict(X_train) ))\n",
        "            self.errors.append(error)\n",
        "\n",
        "    def predict(self, X):\n",
        "        #print(X.shape)\n",
        "        return np.dot(X, self.W) + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vu2rfe9zmt-D"
      },
      "outputs": [],
      "source": [
        "def score(y, y_pred):\n",
        "    r2 = 1 - (np.sum(np.square(y-y_pred))/np.sum(np.square(y-np.mean(y))))\n",
        "    return r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3GZcqFp1Ggb"
      },
      "source": [
        "# Linear Regression - NE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TCwtw1m8mums"
      },
      "outputs": [],
      "source": [
        "class LinearRegression_NE:\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "\n",
        "        #self.m -> no. of training samples( rows )\n",
        "        #self.d -> no. of features( cols )\n",
        "        self.m, self.d = self.X_train.shape\n",
        "        print(self.X_train.shape)\n",
        "        self.X_train = pd.concat(( pd.DataFrame(np.ones((self.m,1))), self.X_train.reset_index(drop=True)), axis = 1)\n",
        "        print(self.X_train.shape)\n",
        "        # inv(X_t.X).X_t.y\n",
        "        self.W = pd.DataFrame(np.dot(np.linalg.inv(np.dot(self.X_train.T, self.X_train)),np.dot(self.X_train.T, self.y_train)))\n",
        "        # self.W = np.linalg.inv(self.X_train.T @ self.X_train) @ (self.X_train.T @ self.y_train)\n",
        "\n",
        "        self.b = self.W.iloc[0,0]\n",
        "        self.W = self.W.iloc[1:,:]\n",
        "\n",
        "    def predict(self, X):\n",
        "        #print(X.shape)\n",
        "        return np.dot(X, self.W) + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmsgQ3VNzY-1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def L2_reg_update_weights(w, b, X, Y, learning_rate, lambda_value=1):\n",
        "    \"\"\"input: w is the coefficient of x in form of an integer.\n",
        "                b is the constant term, integer format.\n",
        "                X is a python list of input variables.\n",
        "                Y is a python list of the output variables.\n",
        "                learning_rate is in the form of float.\n",
        "                lambda_value is an integer which is actually the penalty term in l2 regularization.\n",
        "        output: You are required to return updated m and b upto two decimal points, in the same line.\n",
        "    \"\"\"\n",
        "    # print('w :', w)\n",
        "    # print('b :', b)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    # print('X :', X)\n",
        "    # print('Y :', Y)\n",
        "    Y_hat = np.dot(X, w) + b\n",
        "    # print('Y_hat :', Y_hat)\n",
        "    w_deriv = (np.dot((-X.T), (Y - Y_hat)))/X.shape[0] + (lambda_value * 2 * np.sum(w))\n",
        "    b_deriv = (-1) * np.sum(Y - Y_hat)/X.shape[0]\n",
        "    # print('w_deriv :', w_deriv)\n",
        "    # print('b_deriv :', b_deriv)\n",
        "    # Update weights\n",
        "    w -= learning_rate * w_deriv\n",
        "    b -= learning_rate * b_deriv\n",
        "    # print('w :', w)\n",
        "    # print('b :', b)\n",
        "    # print(np.round(w, 2), np.round(b, 2))\n",
        "    return (np.round(w, 2), np.round(b, 2))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM/7A3KDDmEcUe5anpf6Pdj",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
